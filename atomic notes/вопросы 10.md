Tags: [[для собеседований]]

#golang 



## Планировщик



1. Зачем нужен отдельный планировщик?
   
	Для подобного вопроса на собесе используй заготовленный список ниже
	
	![[Screenshot From 2025-10-07 21-33-46.png]]
	
	
	В планировщие содержатся пулы для различных тяжеловесных структур и не только:
	
	- Пул потоков ОС
	- Пул логических процессоров
	- Пул горутин
	  
	Все эти структуры нам пересоздавать не обязательно
	
	Если нам нужен новый поток - мы не идем к ОС за выделением, а исопльзуем тот, что в пуле
	
	
	Кроме этого планировщик позволяет эффективно работать с **асинхронными syscall** - ми, в частности сетевыми вызовами за счет **netpoller**-а


---


2. Особенности планировщика

	Планировщик GO реализует **NxM планировщик**, т.е. такой, где **на определенное кол-во потоков ОС мапится какое-то кол-во горутин**
	
	Кол-во горутин может быть гораздо больше кол-ва потоков
	
	
	В рамках планировщика используется **GMP - модель**:
	
	- G - горутины (что исполняем)
	
	- M - где исполняется, по сути надстройка над потоком ОС
	  
	- P - "логическое ядро", это абстракция, которая **хранит очередь горутин одного потока и ресурсы** - различные кэши для потока
	
	![[Screenshot From 2025-10-08 01-20-24.png]]
	
	Чтобы уменьшить кол-во переключений контекста **по-умолчанию кол-во лог. ядер в GMP-модели равно кол-ву лог. ядер процессора**
	
	
	
	Кэшами в рамках лог. ядра P служат:
	
	- Кэш стека (stack cache)
	- Кэш кучи (malloc cache)
	- Другие кэши
	
	![[Screenshot From 2025-10-08 01-21-22.png]]
	
	Кэши лог. ядер - это оптимизация, позволяющиая аллокатору **не ходить в общую память процесса**, а **выделять ее в кэше**
	
	Без них каждый поток ждал бы остальные под мьютексом в очереди


---


3. Как новые горутины попадют в очередь? Как они выполняются?

	Изначально при старте программы у нас работает только горутина `main`, в которой непосредственно вызывается ф-ия `main`
	
	Ф-ия `main` запускается **вместе с планировщиком** 
	
	
	
	Новые горутины, порождаемые через `go` **из другой горутины**, приобретают  статус **ready** - т.е. готовый к работе - после назначения ее лог. процессору
	
	После этого они попадают в **локальную очередь**, соотв. потоку, в котором они были созданы
	
	![[Screenshot From 2025-10-08 02-09-42.png]]
	
	
	Как только горутина дошла до **точки выхода** - это может быть, например,  `wg.Wait()` - она сигнализирует планировщику, что ее можно снять с выполнения
	
	Она меняет статус с **Running** на **Waiting**
	
	После этого она снимается с выполнения, и вместо нее выполняется первая горутина из очереди
	
	![[Screenshot From 2025-10-08 02-14-59.png]]
	
	
	Когда все горутины из очереди отработали на потоке ОС, гогутина `main` меняет статус на **Ready**, попадает в ту же самую очередь и выполняется на том же потоке
	
	![[Screenshot From 2025-10-08 02-17-57.png]]
	
	
	Для нескольких потоков ОС все эти процессы повторяются так же
	
	![[Screenshot From 2025-10-08 02-20-03.png]]


> [!info]  
> В статусе **Waiting** у горутин есть причина остановки `waitReason`
> 
> Помимо упомянутой `wg.Wait()` есть еще 40+ значений
> 
> В зависимости от значения `waitReason` планировщик принимает решение, что делать с горутиной
> 
> Вот снэпшот некоторых других значений
> 
> ![[Screenshot From 2025-10-08 02-58-52.png]]
> 


---


4. Как потоки взаимодействуют друг с другом?

	Рано или поздно **в очереди могут закончиться горутины**
	
	Чтобы поток не простаивал, он может **взять из другой локальной очереди несколько горутин**
	
	Очередь, из которой будут красться горутины, выбирается **рандомно**
	
	
	
	Делает он это так: он берет **половину** горутин **с конца очереди**
	
	Именно половину, потому что нужна золотая середина, чтобы **не выполнить все сразу** и **оставить что-то** другой очереди
	
	![[Screenshot From 2025-10-08 02-32-24.png]]
	
	
	
	Помимо локальных очередей есть еще **глобальная очередь**
	
	Из нее потоки тоже могут горутины, но при одном из условий:
	 
	- В локальной очереди **нет горутин**
	  
	- Локальная очередь очень длинная - тогда **после каждой 61-й горутины** мы берем **по одной** из глобальной очереди
	  
	![[Screenshot From 2025-10-08 02-48-33.png]]


> [!hint] 
> Приоритетность поиска горутин для выполнения у процессора выглядит так
> 
> - Если в потоке есть горутины, то через каждые 60 горутин идем в глоб. очередь
> >
> - Берем след. горутины из локальной очереди
> >
> - Если в своей очереди ничего нет, идем брать у другой локальной очереди
> >
> - Только после этого идем в глобальную очередь 
> 


---


5. CS у горутин

	Переключение между горутинами - это **дешевая** операция
	
	Для смены контекста горутинам нужен саиый минимум: просто сохранение/загрузка пары регистров и смена стека; обычно **наносекунды–сотни наносекунд**.
	
	
	К тому же переключение не прерывает никаких расчетов:
	
	1.  Горутины переходят в состояние **waiting** по одной из нескольких причин из 
	 `waitReason`, например из-за мьютекса
	
	2. Потом мы ставим на выполнение следующую в очереди горутину
	
	3. После выполнение мьютекс разблокируется -> вытесненная горутина переходит из сотояния **waiting** в **ready** 
	   
	4. Она возвращается в очередь, небольшой контекст восстанавливается и горутина отрабатывает до конца
	   
	   
	Все это происходит **без переключения потоков и syscall-ов**


---


6. Синхронный syscall
   
	Когда горутине нужно выполнить syscall, сама она напрямую не может пойти в **kernel space** и выполнить код 
	 
	Это может сделать только **планировщик ОС**, который в свою очередь **ничего не значет о горутинах**, он управляет только **потоками ОС**
	
	Поэтому syscall из любой горутины в потоке - это CS в свой поток 
	
	**Поток** же во время syscall **блокируется**
	
	![[Screenshot From 2025-10-08 21-37-43.png]]
	
	
	
	Чтобы другие горутины **не голодали** из-за блокировки мы можем добавить оптимизации в зависимости от **конкретного** syscall
	
	Кол-во syscall в системе **не так много**, им можно даже присвоить индексы
	
	Поэтому компилятор может решить, какую инструкцию ему нужно выполнить **до и после выполнения** syscall
	
	
	
	Если syscall короткий, ты мы даем ему примерно **10мс** на выполнение
	
	Если syscall долгий, то идем по порядку:
	
	- Как только поток блокируется, он "открепляется" от очереди (по сути G, M, P - это **структуры, которые связаны ссылками**, как в связанном списке), т.е. стирает значение ссылки на другую структуру
	
	- После этого он создает новый поток ОС и закрепляет его на свою бывшую очередь, чтобы горутины дальше могли выполняться
	  
	- После создания нового потока старый просто выполняет syscall и переключает контекст на горутину
	  
	- Горутина **возвращается в свой поток**, если есть место или лог. процессор не занят горутиной, **или отправляется в глоб. очередь**
	  
	  
	Весь этот процесс называется **hand-off**
	
	![[Screenshot From 2025-10-08 21-27-38.png]]
	
	
	
	Все созданные таким образом потоки для оптимизации отправляются в **thread pool** - буфер для освобожденных потоков после syscall-а
	
	И если в каком-то из потоков снова случится блокировка, мы заменим поток, закрепленный на свою очередь, **потоком из thread pool**
	
	Такая оптимизация позволяет не создавать дорогостоящие потоки ОС
	
	![[Screenshot From 2025-10-08 21-49-15.png]]


---


7. Асинхронный syscall (netpoller)

	Файловых дескрипторов обычно намного меньше, чем сетевых подключений
	
	Мы можем позволить себе создавать новые потоки **для работы с файлами**, но в случае с **сокетами** такое решение может быть очень дорогим
	
	
	
	Каждый **вызов API** приложения - это **потенциальный поход в БД**, т.е. также **сетевой вызов**
	
	С синхронными запросами мы бы либо:
	
	 - ждали, пока сами горутины освободятся и подвинут локальную очередь вперед  
	
	- либо делали **handoff** - брали потоки из thread pool/создавали новые, а горутины оставались заблокированными в своих потоках
	
	![[Screenshot From 2025-10-09 03-44-19.png]]
	
	
	
	Для подобных сценариев есть специальная структура - **netpoller**
	
	Netpoller - это мультиплексор, которому можно дать файловые дескрипторы (в нашем случае - сокеты)
	
	И когда на каком-либо дескрипторе **происходит событие**, то при вызове syscall-а
	`epoll-wait` (мультиплексором в линуксе служит компонент с именем **epoll**) мы **получим отсчет, на каких сокетах они случились**
	
	
	**Netpoller работает на одном единственном потоке ОС**
	
	**Все** горутины, которые открывают соединения, буду попадать на этот поток
	
	
	
	Netpoller работает по-приинципу **event-loop**
	
	Т.е. один поток последовательно будет выполнять операции из **call stack**
	
	![[Screenshot From 2025-10-09 04-00-59.png]]
	
	
	
	Как отрабатывает netpoller по шагам:
	
	-  В netpoller попадают горутины, которым нужно создать сокет - вызываются `socket()` **поочередно**
	   
	![[Screenshot From 2025-10-09 04-05-19.png]]
	
	
	-  После каждого вызова в **callback queue** создается callback ф-ия, которая будет вызывать `connect()` на созданный сокет
	
	![[Screenshot From 2025-10-09 04-09-45.png]]
	
	
	- 
	
	


---


## Socket (пререквизит для netpoller-а)



8. Синхронные сокеты 

	Когда приложение отправляет сетевой запрос на удаленный сервер, само по себе оно это сделать не может
	
	В рамках TCP/IP протокола **за транспортный уровень отвечает ОС**
	
	Т.е. чтобы отправить запрос нам нужно обратиться к ОС через syscall-ы
	
	
	Для линукса таких syscall будет несколько:
	
	![[Screenshot From 2025-10-08 23-22-09.png]]
	
	
	
	В случае синхронного сокета будет происходить следующее по-порядку:
	
	- Приложение обращается к ОС через метод `socket()`, создается сокет с незанятым портом, обычно это просходит **за короткое время**
	  
	![[Screenshot From 2025-10-08 23-38-00.png]]  
	
	- Через метод `connect()` подключиться к серверу через сокет
	  
	  Во время вызова мы будем делать поход по сети - **это может быть долго**, если сервер далеко от нас
	
	![[Screenshot From 2025-10-08 23-41-05.png]]
	
	
	- После коннекта отправляется syscall `send()` с методом, payload и т.д. После него ОС отправляет запрос к серверу через наш сокет
	  
	![[Screenshot From 2025-10-08 23-43-16.png]]  
	  
	- После ответа от сервера сокет сигнализирует о том, что пришел ответ. Приложение отправляет последний syscall `recv()`, ждет ответ от сервера и приложение получает ответ
	  
	![[Screenshot From 2025-10-08 23-48-28.png]]


---


9. Асинхронные сокеты

	Потенциально у приложения могут быть открыты **десятки тысяч соединейний**
	
	Т.е. потенциально нам потребуется создавать тысячи потоков ОС, а это **большие затраты на память** (на 10К соединений нужно 80Гб памяти)
	
	Это настолько распространенная проблема, что в некоторых языках по дефолту для сетевых вызовов используется **асинхронный сокет** - в GO, JS и др.
	
	
	Для таких сокетов сетевой вызов н**е будет блокировать поток**
	
	Как это работает:
	
	- Таким же методом `socket()` создается сокет (или даже несколько)
	  
	![[Screenshot From 2025-10-09 00-13-28 1.png]]
	
	
	- Приложение вызывает `connect()` для одного/несокльких сокетов. ОС сама коннектится к удаленному серверу, **приложению не нужно никого ждать**
	  
	![[Screenshot From 2025-10-09 00-16-32.png]]
	
	
	- Через syscall `epoll_add()` приложение говорит ОС, на каких сокетах ждать ответ. В самом методе дополнительно передаются ссылки на сокеты
	  
	![[Screenshot From 2025-10-09 00-18-45.png]]
	  
	  
	- Через какое-то время приложение может вызвать syscall `epoll_wait()`, и ОС в ответ отдаст список сокетов, на которых поменялся статус ответа
	  
	![[Screenshot From 2025-10-09 00-23-12.png]]
	
	
	- Если список непустой, приложение может послать syscall `send()` на один или сразу на все сокеты с соотв. методами и payload. При этом, опять же, **приложение не будет ждать ответа** и будет выполнять инструкции дальше
	  
	![[Screenshot From 2025-10-09 00-26-47.png]]  
	  
	- После **нескольких** `epoll-wait()` сокеты будут обновляться, и по мере изменения статуса ответа приложения будет либо отправлять запрос, либо получать ответ
	  
	![[Screenshot From 2025-10-09 00-30-47.png]]

  
---


## Related


