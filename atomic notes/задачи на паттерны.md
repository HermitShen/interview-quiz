Tags: [[для собеседований]]

#golang 



# Паттерны
 

## Fan-in
 

Условие выглядит так:

![[Screenshot From 2025-10-16 01-57-39.png]]
 

От нас хотят реализовать **один воркер**, который будет читать из **нескольких каналов**

Это часть большого паттерна **Fan-out_fan-in**
 

В идеале нужно укладываться **менее чем за 10 минут**
 

Возможная реализация ф-ии

```go
func fanin(ctx context.Context, chans ...<-chan int) <-chan int {  
    out := make(chan int)  
  
    go func() {  
       wg := &sync.WaitGroup{}  
       for _, ch := range chans {  
          wg.Add(1)  
          go func() {  
             for {  
                select {  
                case <-ctx.Done():  
                   return  
                case v, ok := <-ch:  
                   if !ok {  
                      return  
                   }  
                   select {  
                   case out <- v:  
                   case <-ctx.Done():  
                      return  
                   }  
                }             
            }    
        }()       
     }       
     wg.Wait()  
     close(out)  
  
    }()  
    
    return out  
}
```

Здесь был добавлен контекст с таймаутом, если попросят на собесе
 

Нюансы:

- Мы создаем канал `out` **двунаправленным**, потмоу что на выходе он автоматически конвертируется в read-only канал
- Если контекст не нужен, замени `select ` на обычный `range` по каналу
- Абсолютно весь код оберни в горутину **между созданием и возвратом канала** `out`
 
 

## Errgroup
 

На собеседовании могут попросить распараллелить какую-то работу с удаленным источником так, чтобы горутины возвращали ошибку

Ошибка должна **отменять все остальные горутины** в процессе

Оптимальным выбором будет не использовать свою реализацию с примитивами, а использовать метод `{go}errgroup.Go` из пакета `{go}"errgroup"`

 
Одна из возможных реализаций на примере конкуретной записи в in-memory хранилище

```go
type User struct {  
    Name string  
}  
  
// fetch simulates fetching user data from an external source with a delay.  
func fetch(ctx context.Context, user User) (string, error) {  
    ch := make(chan any)  
  
    go func() {  
       time.Sleep(time.Millisecond * 10)  
       close(ch)  
    }()  
    select {  
    case <-ch:  
       return user.Name, nil  
    case <-ctx.Done():  
       return "", errors.New("context cancelled")  
    }}  
  
func process(ctx context.Context, users []User) (map[string]int64, error) {  
    names := make(map[string]int64, 0)  
    mu := &sync.Mutex{}  
  
    egroup, ectx := errgroup.WithContext(ctx)  
    egroup.SetLimit(1000)
  
    for _, user := range users {  
       egroup.Go(func() error {  
          name, err := fetch(ectx, user)  
          if err != nil {  
             return err  
          }  
  
          mu.Lock()  
          defer mu.Unlock()  
          names[name]++  
  
          return nil  
       })  
    }  
    if err := egroup.Wait(); err != nil {  
       return nil, err  
    }  
  
    return names, nil  
}  
  
func main() {  
    names := []User{  
       {"Ann"},  
       {"Bob"},  
       {"Cindy"},  
       {"Bob"},  
    }  
    ctx := context.Background()  
  
    res, err := process(ctx, names)  
    if err != nil {  
       fmt.Println("an error occured", err)  
    }  
    fmt.Println(res)  
}
```
 

Нюансы:

- `{go}errgroup` объединяет в себе ф-ии как **отмены контекста**, так и `{go}waitgroup`, т.е. перед отменой можно дождаться окончания других горутин
  
- Если в горутине вызывается другая ф-ия, мы все равно обязаны проверять контекст в ней отдельно через `{go}select`
  
- метод `{go}SetLimit` может ограничивать кол-во одновременно работающих горутин, чтобы **не нагружать планировщик** и **экономить память**

 
 
## Rate-limiter
 
Какие могут быть условия на собесе:

- Ограниченное кол-во коннектов
- Ограниченное кол-во работающих горутин
- Ограничение RPS

В зависимости от условия нужно брать соотв. паттерн

**Если условие не обговорено зараннее, о нем нужно спросить интервьюера**
 

|       Цель       |   Паттерн   |
|:----------------:|:-----------:|
| Кол-во коннектов | Worker-poll |
|  Кол-во горутин  |   Семафор   |
|       RPS        |    Тикер    |
 
### WP 
 
Если в условии дан слайс, приведи его к каналу через хелпер ф-ию `{go}generate()` и передай в worker-pool

```go
func generate(reqs []Request) chan Request {  
    ch := make(chan Request)  
  
    go func() {  
       defer close(ch)  
       for _, v := range reqs {  
          ch <- v  
       }  
    }()    
    
    return ch  
}

var maxConnects = 10  
  
func (c *client) WithLimiter(ctx context.Context, requests chan Request) {  
    wg := &sync.WaitGroup{}  
  
    wg.Add(maxConnects)  
    for range maxConnects {  
	    go func() {  
		    defer wg.Done() 
		     
		    for req := range requests {  
			    c.SendRequest(ctx, req)  
			}       
		}()    
     }  
     
    wg.Wait()  
}
```
 

### Семафор
 
Здесь можно обойтись слайсом
```go
var maxGoroutins = 100

func (c *client) WithLimiter(ctx context.Context, requests []Request) {  
    tokens := make(chan struct{}, maxGoroutins)  
  
    go func() {  
       for range maxGoroutins {  
          tokens <- struct{}{}  
       }    
	}()  
    for _, req := range requests {  
       <-tokens  
       go func() {  
          defer func() {  
             tokens <- struct{}{}  
          }()          
          c.SendRequest(ctx, req)  
       }()    
     }
     
     // Wait for all goroutines to finish  
	for range maxGoroutins {  
	    <- tokens  
	}
}
```
 
Нюансы:

- Перед запуском горутин хранилище токены нужно **обязательно заполнить**
- В конце синхронизируем горутины через чтение всех токенов `{go}<- tokens`
 
### Тикер
 
Есть 2 варианта:

- Простой вариант через тикер
- С бакетом (burst) - разрешены всплески запросов, но после них интенсивность такая же, как с тикером
 

Простой вариант: 
```go
var rps = 100  
  
func (c *client) WithLimiter(ctx context.Context, requests []Request) {  
    ticker := time.NewTicker(time.Second / time.Duration(rps))  
    wg := &sync.WaitGroup{}  
  
    wg.Add(len(requests))  
    for _, req := range requests {  
       <-ticker.C  
       go func() {  
          defer wg.Done()  
          c.SendRequest(ctx, req)  
       }()    
     }  
     
    wg.Wait()  
}
```
 
Burst:
```go
var rps = 20  
var burst = 10  
  
func (c *client) WithLimiter(ctx context.Context, requests []Request) {  
    ticker := time.NewTicker(time.Second / time.Duration(rps))  
    tickets := make(chan struct{}, burst)  
  
    wg := &sync.WaitGroup{}  
  
    // Fill the burst tokens  
    go func() {  
       for range burst {  
          tickets <- struct{}{}  
       }    
	}()
	  
    // Refill tokens at the rate of rps  
    go func() {  
       for {  
          select {  
          case <-ticker.C:  
             tickets <- struct{}{}  
          }       
        }    
	}()  
     
    wg.Add(len(requests))  
    for _, req := range requests {  
       <-tickets  
       go func() {  
          defer wg.Done()  
          c.SendRequest(ctx, req)  
       }()    
     }
       
    wg.Wait()  
}
```
 
 


## HTTP
 
На собесе могут попросить распараллелить походы на удаленные серверы с доп. задачами - например собраться все статус-коды в in-memory хранилище



Самой простой подход - создать воркеры с ограничением на кол-во коннектов

 Коннекты будем лимитировать через worker pool
 

```go
func main() {  
    urls := []string{  
       "https://google.com",  
       "https://yandex.ru",  
       "https://google.com",  
       "https://yandex.ru",  
       "https://google.com",  
       "https://yandex.ru",  
       "https://google.com",  
       "https://yandex.ru",  
       "https://google.com",  
       "https://yandex.ru",  
       "https://amazon.com",  
       "https://youtube.com",  
    }  
    fmt.Println(process(urls))  
}  
  
var client http.Client  
  
var maxConnects = 10  
  
func process(urls []string) map[int]int {  
    // make fixed-size map to avoid evacuations during concurrent writes  
    statusCodeCounts := make(map[int]int, len(urls))  
    wg := &sync.WaitGroup{}  
    mu := &sync.Mutex{}  
  
    // channel to distribute URLs among workers  
    ch := make(chan string)  
    go func() {  
       defer close(ch)  
       for _, url := range urls {  
          ch <- url  
       }  
    }()  
    
    // we separate the URL processing logic into its own function  
    // to prevent deferring context cancellation only once per goroutine    
    processUrl := func(url string) {  
       ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)  
       defer cancel()  
  
       req, _ := http.NewRequestWithContext(ctx, http.MethodGet, url, nil)  
  
       resp, err := client.Do(req)  
       if err != nil {  
          fmt.Println("error occured", err.Error())  
          return  
       }  
  
       mu.Lock()  
       defer mu.Unlock()  
       statusCodeCounts[resp.StatusCode]++  
    }  
  
    wg.Add(maxConnects)  
    for range maxConnects {  
       go func() {  
          defer wg.Done()  
  
          for url := range ch {  
             processUrl(url)  
          }       
        }()    
     }  
     
    wg.Wait()  
    return statusCodeCounts  
}
```
 
Нюансы:

- Клиент мы выносим за ф-ию обработки реквестов, чтобы не аллоцировать под нее память на каждой отправке
  
- Отдельная ф-ия под реквест, чтобы контекст отменялся **после каждого запроса**, а не под конец жизни горутины
  
- Фиксированный размер мапы чтобы **избежать эвакуации данных**
 
 

## Настоящий WP
 
Здесь представлена реализация WP с самим буфером

```go
func say(id int, phrase string) {  
    time.Sleep(20 * time.Millisecond)  
    fmt.Printf("Worket %d says: %s\n", id, phrase)  
}  
  
func makePool(poolSize int, handler func(int, string)) (func(string), func()) {  
    pool := make(chan int, poolSize)  
  
    for i := range poolSize {  
       pool <- i  
    }  
  
    handle := func(s string) {  
       id := <-pool  
       go func() {  
          defer func() {  
             pool <- id  
          }()  
  
          handler(id, s)  
       }()    }  
    wait := func() {  
       for range poolSize {  
          <-pool  
       }  
    }  
    return handle, wait  
}  
  
func main() {  
    phrases := []string{}  
  
    for i := range 100 {  
       phrases = append(phrases, fmt.Sprintf("phrase %d", i))  
    }  
    handle, wait := makePool(5, say)  
  
    for _, phrase := range phrases {  
       handle(phrase)  
    }  
    wait()  
}
```
 
Нюансы:

- Это не идеальная реализация, одна и та же горутина может быть вызвана из пула несколько раз подряд, но для собеса некритично
  
- В конце обязательно синхронизируй все воркеры через чтение из пула, иначе `{go}main` вернется до завершения горутин
 
