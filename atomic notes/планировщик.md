Tags: [[для собеседований]]

#golang 



# Планировщик
___



Про какие особенности планировщика могут спросить в глубину

![[Screenshot From 2025-10-10 00-08-23.png]]



## Зачем нужен отдельный планировщик?
---

   
Для подобного вопроса на собесе используй заготовленный список ниже

![[Screenshot From 2025-10-07 21-33-46.png]]


В планировщие содержатся пулы для различных тяжеловесных структур и не только:

- Пул потоков ОС
- Пул логических процессоров
- Пул горутин
  
Все эти структуры нам пересоздавать не обязательно

Если нам нужен новый поток - мы не идем к ОС за выделением, а исопльзуем тот, что в пуле


Кроме этого планировщик позволяет эффективно работать с **асинхронными syscall** - ми, в частности сетевыми вызовами за счет **netpoller**-а



## Особенности планировщика
---


Планировщик GO реализует **NxM планировщик**, т.е. такой, где **на определенное кол-во потоков ОС мапится какое-то кол-во горутин**

Кол-во горутин может быть гораздо больше кол-ва потоков


В рамках планировщика используется **GMP - модель**:

- G - горутины (что исполняем)

- M - где исполняется, по сути надстройка над потоком ОС
  
- P - "логическое ядро", это абстракция, которая **хранит очередь горутин одного потока и ресурсы** - различные кэши для потока

![[Screenshot From 2025-10-08 01-20-24.png]]

Чтобы уменьшить кол-во переключений контекста **по-умолчанию кол-во лог. ядер в GMP-модели равно кол-ву лог. ядер процессора**



Кэшами в рамках лог. ядра P служат:

- Кэш стека (stack cache)
- Кэш кучи (malloc cache)
- Другие кэши

![[Screenshot From 2025-10-08 01-21-22.png]]

Кэши лог. ядер - это оптимизация, позволяющиая аллокатору **не ходить в общую память процесса**, а **выделять ее в кэше**

Без них каждый поток ждал бы остальные под мьютексом в очереди



## Как новые горутины попадют в очередь? Как они выполняются?
---


Изначально при старте программы у нас работает только горутина `main`, в которой непосредственно вызывается ф-ия `main`

Ф-ия `main` запускается **вместе с планировщиком** 



Новые горутины, порождаемые через `go` **из другой горутины**, приобретают  статус **ready** - т.е. готовый к работе - после назначения ее лог. процессору

После этого они попадают в **локальную очередь**, соотв. потоку, в котором они были созданы

![[Screenshot From 2025-10-08 02-09-42.png]]


Как только горутина дошла до **точки выхода** - это может быть, например,  `wg.Wait()` - она сигнализирует планировщику, что ее можно снять с выполнения

Она меняет статус с **Running** на **Waiting**

После этого она снимается с выполнения, и вместо нее выполняется первая горутина из очереди

![[Screenshot From 2025-10-08 02-14-59.png]]


Когда все горутины из очереди отработали на потоке ОС, гогутина `main` меняет статус на **Ready**, попадает в ту же самую очередь и выполняется на том же потоке

![[Screenshot From 2025-10-08 02-17-57.png]]


Для нескольких потоков ОС все эти процессы повторяются так же

![[Screenshot From 2025-10-08 02-20-03.png]]


> [!info]  
> В статусе **Waiting** у горутин есть причина остановки `waitReason`
> 
> Помимо упомянутой `wg.Wait()` есть еще 40+ значений
> 
> В зависимости от значения `waitReason` планировщик принимает решение, что делать с горутиной
> 
> Вот снэпшот некоторых других значений
> 
> ![[Screenshot From 2025-10-08 02-58-52.png]]
> 



## Как потоки взаимодействуют друг с другом?
---


Рано или поздно **в очереди могут закончиться горутины**

Чтобы поток не простаивал, он может **взять из другой локальной очереди несколько горутин**

Очередь, из которой будут красться горутины, выбирается **рандомно**



Делает он это так: он берет **половину** горутин **с конца очереди**

Именно половину, потому что нужна золотая середина, чтобы **не выполнить все сразу** и **оставить что-то** другой очереди

![[Screenshot From 2025-10-08 02-32-24.png]]



Помимо локальных очередей есть еще **глобальная очередь**

Из нее потоки тоже могут горутины, но при одном из условий:
 
- В локальной очереди **нет горутин**
  
- Локальная очередь очень длинная - тогда **после каждой 61-й горутины** мы берем **по одной** из глобальной очереди
  
![[Screenshot From 2025-10-08 02-48-33.png]]


> [!hint] 
> Приоритетность поиска горутин для выполнения у процессора выглядит так
> 
> - Если в потоке есть горутины, то через каждые 60 горутин идем в глоб. очередь
> >
> - Берем след. горутины из локальной очереди
> >
> - Если в своей очереди ничего нет, идем брать у другой локальной очереди
> >
> - Только после этого идем в глобальную очередь 
> >
> - Sysmon опрашивает netpoller
> 



## CS у горутин
---


Переключение между горутинами - это **дешевая** операция

Для смены контекста горутинам нужен саиый минимум: просто сохранение/загрузка пары регистров и смена стека; обычно **наносекунды–сотни наносекунд**.


К тому же переключение не прерывает никаких расчетов:

1.  Горутины переходят в состояние **waiting** по одной из нескольких причин из 
 `waitReason`, например из-за мьютекса

2. Потом мы ставим на выполнение следующую в очереди горутину

3. После выполнение мьютекс разблокируется -> вытесненная горутина переходит из сотояния **waiting** в **ready** 
   
4. Она возвращается в очередь, небольшой контекст восстанавливается и горутина отрабатывает до конца
   
   
Все это происходит **без переключения потоков и syscall-ов**



## Синхронный syscall
---

   
Когда горутине нужно выполнить syscall, сама она напрямую не может пойти в **kernel space** и выполнить код 
 
Это может сделать только **планировщик ОС**, который в свою очередь **ничего не значет о горутинах**, он управляет только **потоками ОС**

Поэтому syscall из любой горутины в потоке - это CS в свой поток 

**Поток** же во время syscall **блокируется**

![[Screenshot From 2025-10-08 21-37-43.png]]



Чтобы другие горутины **не голодали** из-за блокировки мы можем добавить оптимизации в зависимости от **конкретного** syscall

Кол-во syscall в системе **не так много**, им можно даже присвоить индексы

Поэтому компилятор может решить, какую инструкцию ему нужно выполнить **до и после выполнения** syscall



Если syscall короткий, ты мы даем ему примерно **10мс** на выполнение

Если syscall долгий, то идем по порядку:

- Как только поток блокируется, он "открепляется" от очереди (по сути G, M, P - это **структуры, которые связаны ссылками**, как в связанном списке), т.е. стирает значение ссылки на другую структуру

- После этого он создает новый поток ОС и закрепляет его на свою бывшую очередь, чтобы горутины дальше могли выполняться
  
- После создания нового потока старый просто выполняет syscall и переключает контекст на горутину
  
- Горутина **возвращается в свой поток**, если есть место или лог. процессор не занят горутиной, **или отправляется в глоб. очередь**
  
  
Весь этот процесс называется **hand-off**

![[Screenshot From 2025-10-08 21-27-38.png]]



Все созданные таким образом потоки для оптимизации отправляются в **thread pool** - буфер для освобожденных потоков после syscall-а

И если в каком-то из потоков снова случится блокировка, мы заменим поток, закрепленный на свою очередь, **потоком из thread pool**

Такая оптимизация позволяет не создавать дорогостоящие потоки ОС

![[Screenshot From 2025-10-08 21-49-15.png]]



## Асинхронный syscall (netpoller)
---


Файловых дескрипторов обычно намного меньше, чем сетевых подключений

Мы можем позволить себе создавать новые потоки **для работы с файлами**, но в случае с **сокетами** такое решение может быть очень дорогим



Каждый **вызов API** приложения - это **потенциальный поход в БД**, т.е. также **сетевой вызов**

С синхронными запросами мы бы либо:

 - ждали, пока сами горутины освободятся и подвинут локальную очередь вперед  

- либо делали **handoff** - брали потоки из thread pool/создавали новые, а горутины оставались заблокированными в своих потоках

![[Screenshot From 2025-10-09 03-44-19.png]]



Для подобных сценариев есть специальная структура - **netpoller**

Netpoller - это мультиплексор, которому можно дать файловые дескрипторы (в нашем случае - сокеты)

И когда на каком-либо дескрипторе **происходит событие**, то при вызове syscall-а
`epoll-wait` (мультиплексором в линуксе служит компонент с именем **epoll**) мы **получим отсчет, на каких сокетах они случились**


**Netpoller работает на одном единственном потоке ОС**

**Все** горутины, которые открывают соединения, буду попадать на этот поток



Netpoller работает по-приинципу **event-loop**

Т.е. один поток последовательно будет выполнять операции из **call stack**

![[Screenshot From 2025-10-09 04-00-59.png]]



Как отрабатывает netpoller по шагам:

-  В netpoller попадают горутины, которым нужно создать сокет - вызываются `socket()` **поочередно**
   
![[Screenshot From 2025-10-09 04-05-19.png]]


-  После каждого вызова в **callback queue** создается callback ф-ия, которая будет вызывать `connect()` на созданный сокет

![[Screenshot From 2025-10-09 04-09-45.png]]


- Когда вернется адрес сокета от ОС, callback ф-ия положит новую задачу в call stack

![[Screenshot From 2025-10-09 04-12-23.png]]


- Все пункты выше применяеются для всех горутин
  
![[Screenshot From 2025-10-09 04-14-32.png]]


- Все остальные syscall-ы отрабатывают также, дальше в какой-то момент netpoller отправляет syscall `epoll_add`, чтобы ОС следила за сокетами


- В какой-то момент netpoller делает syscall `epoll_wait()`, после чего одни горутины выополняют `send()`, а другие `recv()`

![[Screenshot From 2025-10-09 04-40-22.png]]


- После `recv()` добавляется callback, который вызовет `go runnable`, переводящий горутины из состояние `waiting`, с которой она пришла в epoll, в состояние ready
  
  
- После смены состояния горутина возвращается либо в свою локальную очередь, если она **не заполнена** и **поток все еще существует**, либо в глобальную очередь в случае
  
![[Screenshot From 2025-10-09 04-45-45.png]]


> [!caution] 
> **Для кол-ва файловых дескрипторов в линуксе существует лимит** 
> 
> Если **лимит превышается**, то происходит **ошибка рантайма**
> 
> Увеличить лимит можно на уровне ОС, например через параметр `ulimit` 



## Долгоживущие процессы, sysmon
---



**sysmon** - это отдельный компонент планировщика GO, который выполняет несколько похожих ролей:

- Следит за временем выполнения горутин:
  
	Каждый раз, когда горутина в локальной очереди отправляет syscall, ей дается **10мс** на выполнение. Когда таймер истекает начинается процесс handoff-а
   

- Периодически опрашивает netpoller:

	Если горутины в netpoller-е ждут ответа дольше **10 мс** (легко запомнить тайминг), sysmon отправляет syscall, чтобы ОС сняла их с потока и переместила в глобальную очередь


Т.о. несмотря на то, что планировщик работает по приципу кооперативной многозадачности, он может снять с выполнения работающую горутину **косвенно** (через syscall)



У планировщика есть оптимизация в виде LIFO - буфера

Оптимизация заключается в порядке ввода **дочерних** горутин: **последняя** горутина, созданная внутри другой горутины в очереди, **попадает не в run queue, а LFIO buffer**

Вместо того, чтобы класть ее в конец очереди, лог. процессор выполнит ее **после той горутины, которая ее породила**

![[Screenshot From 2025-10-09 23-57-42.png]]
Здесь G2 породила G4, которая попала в буфер


Если G2 потом породит G5, G4 попадет в очередь, а G5 заменит ее в буфере

![[Screenshot From 2025-10-09 23-59-12.png]]


На родительскую и дочернюю горутины распространяется общий лимит выполнения в **10мс**, на случай если они часто буду переключаться

Если лимит превышен, родительская горутина уходит в глоб. очередь

![[Screenshot From 2025-10-10 00-05-33.png]]



# Socket (пререквизит для netpoller-а)
---



## Синхронные сокеты 
---


Когда приложение отправляет сетевой запрос на удаленный сервер, само по себе оно это сделать не может

В рамках TCP/IP протокола **за транспортный уровень отвечает ОС**

Т.е. чтобы отправить запрос нам нужно обратиться к ОС через syscall-ы


Для линукса таких syscall будет несколько:

![[Screenshot From 2025-10-08 23-22-09.png]]



В случае синхронного сокета будет происходить следующее по-порядку:

- Приложение обращается к ОС через метод `socket()`, создается сокет с незанятым портом, обычно это просходит **за короткое время**
  
![[Screenshot From 2025-10-08 23-38-00.png]]  

- Через метод `connect()` подключиться к серверу через сокет
  
  Во время вызова мы будем делать поход по сети - **это может быть долго**, если сервер далеко от нас

![[Screenshot From 2025-10-08 23-41-05.png]]


- После коннекта отправляется syscall `send()` с методом, payload и т.д. После него ОС отправляет запрос к серверу через наш сокет
  
![[Screenshot From 2025-10-08 23-43-16.png]]  
  
- После ответа от сервера сокет сигнализирует о том, что пришел ответ. Приложение отправляет последний syscall `recv()`, ждет ответ от сервера и приложение получает ответ
  
![[Screenshot From 2025-10-08 23-48-28.png]]



## Асинхронные сокеты
---


Потенциально у приложения могут быть открыты **десятки тысяч соединейний**

Т.е. потенциально нам потребуется создавать тысячи потоков ОС, а это **большие затраты на память** (на 10К соединений нужно 80Гб памяти)

Это настолько распространенная проблема, что в некоторых языках по дефолту для сетевых вызовов используется **асинхронный сокет** - в GO, JS и др.


Для таких сокетов сетевой вызов н**е будет блокировать поток**

Как это работает:

- Таким же методом `socket()` создается сокет (или даже несколько)
  
![[Screenshot From 2025-10-09 00-13-28 1.png]]


- Приложение вызывает `connect()` для одного/несокльких сокетов. ОС сама коннектится к удаленному серверу, **приложению не нужно никого ждать**
  
![[Screenshot From 2025-10-09 00-16-32.png]]


- Через syscall `epoll_add()` приложение говорит ОС, на каких сокетах ждать ответ. В самом методе дополнительно передаются ссылки на сокеты
  
![[Screenshot From 2025-10-09 00-18-45.png]]
  
  
- Через какое-то время приложение может вызвать syscall `epoll_wait()`, и ОС в ответ отдаст список сокетов, на которых поменялся статус ответа
  
![[Screenshot From 2025-10-09 00-23-12.png]]


- Если список непустой, приложение может послать syscall `send()` на один или сразу на все сокеты с соотв. методами и payload. При этом, опять же, **приложение не будет ждать ответа** и будет выполнять инструкции дальше
  
![[Screenshot From 2025-10-09 00-26-47.png]]  
  
- После **нескольких** `epoll-wait()` сокеты будут обновляться, и по мере изменения статуса ответа приложения будет либо отправлять запрос, либо получать ответ
  
![[Screenshot From 2025-10-09 00-30-47.png]]


